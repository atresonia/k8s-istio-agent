# use the ollama model
llm:
  provider: "ollama"
  config:
    model: "mistral:instruct"
    temperature: 0.3
    max_tokens: 1000
    top_p: 0.9
    context_window: 4096
    do_sample: true
    top_k: 0

logging:
  level: "DEBUG"

